{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from warnings import filterwarnings\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {\n",
    "    \"219gbqQt+ML.jpg\": \"height\",\n",
    "    \"218vf17tHkL.jpg\": \"weight\",\n",
    "    \"21-VzxP3BDL.jpg\": \"item_volume\",\n",
    "    \"217V+UhIrHL.jpg\": \"length\",\n",
    "    \"11j0F4QOiFL.jpg\": \"height\",\n",
    "    \"211sXYcOHcL.jpg\": \"height\",\n",
    "    \"218zo3iJ2IL.jpg\": \"length\",\n",
    "    \"213VIsNlvzL.jpg\": \"height\",\n",
    "    \"21+quvMwZSL.jpg\": \"weight\",\n",
    "    \"217+y-mckBL.jpg\": \"weight\",\n",
    "    \"211EIgVhPEL.jpg\": \"voltage\",\n",
    "    \"218tBdpDGPS.jpg\": \"length\",\n",
    "    \"21-V2Kx5BVL.jpg\": \"length\"\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"219gbqQt+ML.jpg\": \"12 cm\",\n",
    "    \"218vf17tHkL.jpg\": \"250 mg\",\n",
    "    \"21-VzxP3BDL.jpg\": \"200 ml\",\n",
    "    \"217V+UhIrHL.jpg\": \"5 cm\",\n",
    "    \"11j0F4QOiFL.jpg\": \"2.75 inches\",\n",
    "    \"211sXYcOHcL.jpg\": \"8 cm\",\n",
    "    \"218zo3iJ2IL.jpg\": \"44.2 cm\",\n",
    "    \"213VIsNlvzL.jpg\": \"11 cm\",\n",
    "    \"21+quvMwZSL.jpg\": \"1.6 lbs\",\n",
    "    \"217+y-mckBL.jpg\": \"400 mg\",\n",
    "    \"211EIgVhPEL.jpg\": \"3.7 V\",\n",
    "    \"218tBdpDGPS.jpg\": \"104.5 inches\",\n",
    "    \"21-V2Kx5BVL.jpg\": \"80 inches\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./donut-finetuned-docvqa\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, x_dict, y_dict, processor):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.images = list(x_dict.keys())\n",
    "        self.questions = list(x_dict.values())\n",
    "        self.answers = list(y_dict.values())\n",
    "        self.pre_finetune_text = 'Given the image, what is the'\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        print(len(self.images), len(self.questions), len(self.answers), len(self.image_files))\n",
    "        assert type(self.answers) == list, \"Answer should be a list of strings\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        question = f\"{self.pre_finetune_text} {self.questions[idx]}?\"\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        # Prepare the inputs for the Donut model\n",
    "        encoding = self.processor(images=image, text=question, return_tensors=\"pt\")\n",
    "        \n",
    "        # Add the answer as the target text\n",
    "        encoding[\"labels\"] = self.processor.tokenizer(answer, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    print('started collating')\n",
    "    \n",
    "    # Initialize dictionaries to store batched data\n",
    "    batched_data = {\n",
    "        'pixel_values': [],\n",
    "        'labels': []\n",
    "    }\n",
    "\n",
    "    max_label_length = max(item['labels'].size(0) for item in batch)\n",
    "\n",
    "    for item in batch:\n",
    "        batched_data['pixel_values'].append(item['pixel_values'])\n",
    "        \n",
    "        # Pad labels to max length in batch\n",
    "        labels = item['labels']\n",
    "        padded_labels = torch.full((max_label_length,), -100, dtype=torch.long)\n",
    "        padded_labels[:labels.size(0)] = labels\n",
    "        batched_data['labels'].append(padded_labels)\n",
    "\n",
    "    # Stack tensors\n",
    "    batched_data['pixel_values'] = torch.stack(batched_data['pixel_values'])\n",
    "    batched_data['labels'] = torch.stack(batched_data['labels'])\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageDataset(\"/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test\", x_dict, y_dict, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id  # or another appropriate token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # tokenizer=processor.tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
