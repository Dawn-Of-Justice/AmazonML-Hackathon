{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained processor and model\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {\n",
    "    \"219gbqQt+ML.jpg\": \"height\",\n",
    "    \"218vf17tHkL.jpg\": \"weight\",\n",
    "    \"21-VzxP3BDL.jpg\": \"item_volume\",\n",
    "    \"217V+UhIrHL.jpg\": \"length\",\n",
    "    \"11j0F4QOiFL.jpg\": \"height\",\n",
    "    \"211sXYcOHcL.jpg\": \"height\",\n",
    "    \"218zo3iJ2IL.jpg\": \"length\",\n",
    "    \"213VIsNlvzL.jpg\": \"height\",\n",
    "    \"21+quvMwZSL.jpg\": \"weight\",\n",
    "    \"217+y-mckBL.jpg\": \"weight\",\n",
    "    \"211EIgVhPEL.jpg\": \"voltage\",\n",
    "    \"218tBdpDGPS.jpg\": \"length\",\n",
    "    \"21-V2Kx5BVL.jpg\": \"length\"\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"219gbqQt+ML.jpg\": \"12 cm\",\n",
    "    \"218vf17tHkL.jpg\": \"250 mg\",\n",
    "    \"21-VzxP3BDL.jpg\": \"200 ml\",\n",
    "    \"217V+UhIrHL.jpg\": \"5 cm\",\n",
    "    \"11j0F4QOiFL.jpg\": \"2.75 inches\",\n",
    "    \"211sXYcOHcL.jpg\": \"8 cm\",\n",
    "    \"218zo3iJ2IL.jpg\": \"44.2 cm\",\n",
    "    \"213VIsNlvzL.jpg\": \"11 cm\",\n",
    "    \"21+quvMwZSL.jpg\": \"1.6 lbs\",\n",
    "    \"217+y-mckBL.jpg\": \"400 mg\",\n",
    "    \"211EIgVhPEL.jpg\": \"3.7 V\",\n",
    "    \"218tBdpDGPS.jpg\": \"104.5 inches\",\n",
    "    \"21-V2Kx5BVL.jpg\": \"80 inches\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "\n",
    "    # Prepare input for the model\n",
    "    task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n",
    "    prompt = task_prompt.replace(\"{user_input}\", question)\n",
    "    \n",
    "    # Tokenize input question\n",
    "    input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids.squeeze()\n",
    "\n",
    "    # Process image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "    # Tokenize answer as label\n",
    "    labels = processor.tokenizer(answer, add_special_tokens=False, return_tensors=\"pt\").input_ids.squeeze()\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, questions, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "        self.question = questions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the folder: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return len(files)\n",
    "\n",
    "folder_path = '/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test'\n",
    "print(f\"Number of files in the folder: {count_files_in_folder(folder_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"path/to/train_dataset\"),  # Update this with your dataset\n",
    "    \"validation\": load_dataset(\"path/to/val_dataset\")  # Update this with your validation dataset\n",
    "})\n",
    "\n",
    "# Preprocess dataset\n",
    "train_dataset = dataset[\"train\"].map(process_example)\n",
    "val_dataset = dataset[\"validation\"].map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,  # Adjust batch size based on available memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./donut-finetuned-docvqa\"\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=processor.data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./donut-finetuned-docvqa\")\n",
    "processor.save_pretrained(\"./donut-finetuned-docvqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
