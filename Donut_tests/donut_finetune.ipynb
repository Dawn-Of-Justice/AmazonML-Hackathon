{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {\n",
    "    \"219gbqQt+ML.jpg\": \"height\",\n",
    "    \"218vf17tHkL.jpg\": \"weight\",\n",
    "    \"21-VzxP3BDL.jpg\": \"item_volume\",\n",
    "    \"217V+UhIrHL.jpg\": \"length\",\n",
    "    \"11j0F4QOiFL.jpg\": \"height\",\n",
    "    \"211sXYcOHcL.jpg\": \"height\",\n",
    "    \"218zo3iJ2IL.jpg\": \"length\",\n",
    "    \"213VIsNlvzL.jpg\": \"height\",\n",
    "    \"21+quvMwZSL.jpg\": \"weight\",\n",
    "    \"217+y-mckBL.jpg\": \"weight\",\n",
    "    \"211EIgVhPEL.jpg\": \"voltage\",\n",
    "    \"218tBdpDGPS.jpg\": \"length\",\n",
    "    \"21-V2Kx5BVL.jpg\": \"length\"\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"219gbqQt+ML.jpg\": \"12 cm\",\n",
    "    \"218vf17tHkL.jpg\": \"250 mg\",\n",
    "    \"21-VzxP3BDL.jpg\": \"200 ml\",\n",
    "    \"217V+UhIrHL.jpg\": \"5 cm\",\n",
    "    \"11j0F4QOiFL.jpg\": \"2.75 inches\",\n",
    "    \"211sXYcOHcL.jpg\": \"8 cm\",\n",
    "    \"218zo3iJ2IL.jpg\": \"44.2 cm\",\n",
    "    \"213VIsNlvzL.jpg\": \"11 cm\",\n",
    "    \"21+quvMwZSL.jpg\": \"1.6 lbs\",\n",
    "    \"217+y-mckBL.jpg\": \"400 mg\",\n",
    "    \"211EIgVhPEL.jpg\": \"3.7 V\",\n",
    "    \"218tBdpDGPS.jpg\": \"104.5 inches\",\n",
    "    \"21-V2Kx5BVL.jpg\": \"80 inches\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Seq2SeqTrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# predict_with_generate=True,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# logging_steps=100,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      9\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     10\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     12\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./donut-finetuned-docvqa\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Seq2SeqTrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./donut-finetuned-docvqa\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, image_dir, x_dict, y_dict, transform=None):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transform = transform\n",
    "#         self.images = list(x_dict.keys())\n",
    "#         self.input_ids = list(x_dict.values())\n",
    "#         self.labels = list(y_dict.values())\n",
    "#         self.pre_finetune_text = '''Given the image, what is the''' # to stay consistent for finetuning\n",
    "#         self.image_files = os.listdir(image_dir)\n",
    "#         print(len(self.images), len(self.input_ids), len(self.labels), len(self.image_files))\n",
    "#         assert type(self.labels) == list, \"Answer should be a list of strings\"\n",
    "        \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "#         image = Image.open(img_name)\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image\n",
    "\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = ImageDataset(\"/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test\", x_dict, y_dict, processor)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./donut-finetuned-docvqa\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# # Define data collator\n",
    "# def collate_fn(batch):\n",
    "#     print('started colating')\n",
    "#     input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "#     attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "#     labels = torch.stack([item['labels'] for item in batch])\n",
    "#     pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "#     print(input_ids.shape, attention_mask.shape, labels.shape, pixel_values.shape)\n",
    "    \n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_mask,\n",
    "#         'labels': labels,\n",
    "#         'pixel_values': pixel_values,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, image_dir, x_dict, y_dict, processor):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.processor = processor\n",
    "#         self.images = list(x_dict.keys())\n",
    "#         self.input_ids = list(x_dict.values())\n",
    "#         self.labels = list(y_dict.values())\n",
    "#         self.pre_finetune_text = 'Given the image, what is the'\n",
    "#         self.image_files = os.listdir(image_dir)\n",
    "#         print(len(self.images), len(self.input_ids), len(self.labels), len(self.image_files))\n",
    "#         assert type(self.labels) == list, \"Answer should be a list of strings\"\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "#         image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "#         question = f\"{self.pre_finetune_text} {self.input_ids[idx]}?\"\n",
    "#         answer = self.labels[idx]\n",
    "        \n",
    "#         encoding = self.processor(images=image, text=question, return_tensors=\"pt\")\n",
    "        \n",
    "#         # Add labels to the encoding\n",
    "#         encoding['labels'] = self.processor.tokenizer(answer, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "#         # Remove the batch dimension\n",
    "#         for k,v in encoding.items():\n",
    "#             encoding[k] = v.squeeze()\n",
    "        \n",
    "#         return encoding\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     print('started collating')\n",
    "#     return processor.pad(batch, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = ImageDataset(\"/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test\", x_dict, y_dict, processor)\n",
    "\n",
    "# # Define trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image\n",
    "# from torch.utils.data import Dataset\n",
    "# import os\n",
    "\n",
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, image_dir, x_dict, y_dict, processor):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.processor = processor\n",
    "#         self.images = list(x_dict.keys())\n",
    "#         self.questions = list(x_dict.values())\n",
    "#         self.answers = list(y_dict.values())\n",
    "#         self.pre_finetune_text = 'Given the image, what is the'\n",
    "#         self.image_files = os.listdir(image_dir)\n",
    "#         print(len(self.images), len(self.questions), len(self.answers), len(self.image_files))\n",
    "#         assert type(self.answers) == list, \"Answer should be a list of strings\"\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.questions)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "#         image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "#         question = f\"{self.pre_finetune_text} {self.questions[idx]}?\"\n",
    "#         answer = self.answers[idx]\n",
    "        \n",
    "#         # Process the image\n",
    "#         pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        \n",
    "#         # Tokenize the question and answer\n",
    "#         question_encoding = self.processor.tokenizer(question, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "#         answer_encoding = self.processor.tokenizer(answer, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "#         return {\n",
    "#             \"pixel_values\": pixel_values,\n",
    "#             \"input_ids\": question_encoding.input_ids.squeeze(),\n",
    "#             \"attention_mask\": question_encoding.attention_mask.squeeze(),\n",
    "#             \"labels\": answer_encoding.input_ids.squeeze()\n",
    "#         }\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     print('started collating')\n",
    "    \n",
    "#     # Initialize dictionaries to store batched data\n",
    "#     batched_data = {\n",
    "#         'pixel_values': [],\n",
    "#         'input_ids': [],\n",
    "#         'attention_mask': [],\n",
    "#         'labels': []\n",
    "#     }\n",
    "\n",
    "#     # Collect data from each item in the batch\n",
    "#     for item in batch:\n",
    "#         batched_data['pixel_values'].append(item['pixel_values'])\n",
    "#         batched_data['input_ids'].append(item['input_ids'])\n",
    "#         batched_data['attention_mask'].append(item['attention_mask'])\n",
    "#         batched_data['labels'].append(item['labels'])\n",
    "\n",
    "#     # Convert lists to tensors and pad where necessary\n",
    "#     batched_data['pixel_values'] = torch.stack(batched_data['pixel_values'])\n",
    "#     batched_data['input_ids'] = torch.nn.utils.rnn.pad_sequence(batched_data['input_ids'], batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "#     batched_data['attention_mask'] = torch.nn.utils.rnn.pad_sequence(batched_data['attention_mask'], batch_first=True, padding_value=0)\n",
    "#     batched_data['labels'] = torch.nn.utils.rnn.pad_sequence(batched_data['labels'], batch_first=True, padding_value=-100)  # -100 is often used for ignored label indices\n",
    "\n",
    "#     return batched_data\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = ImageDataset(\"/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test\", x_dict, y_dict, processor)\n",
    "\n",
    "# # Define trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     data_collator=collate_fn,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, x_dict, y_dict, processor):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.images = list(x_dict.keys())\n",
    "        self.questions = list(x_dict.values())\n",
    "        self.answers = list(y_dict.values())\n",
    "        self.pre_finetune_text = 'Given the image, what is the'\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        print(len(self.images), len(self.questions), len(self.answers), len(self.image_files))\n",
    "        assert type(self.answers) == list, \"Answer should be a list of strings\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        question = f\"{self.pre_finetune_text} {self.questions[idx]}?\"\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        # Prepare the inputs for the Donut model\n",
    "        encoding = self.processor(images=image, text=question, return_tensors=\"pt\")\n",
    "        \n",
    "        # Add the answer as the target text\n",
    "        encoding[\"labels\"] = self.processor.tokenizer(answer, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    print('started collating')\n",
    "    \n",
    "    # Initialize dictionaries to store batched data\n",
    "    batched_data = {\n",
    "        'pixel_values': [],\n",
    "        'labels': []\n",
    "    }\n",
    "\n",
    "    max_label_length = max(item['labels'].size(0) for item in batch)\n",
    "\n",
    "    # Collect data from each item in the batch\n",
    "    for item in batch:\n",
    "        batched_data['pixel_values'].append(item['pixel_values'])\n",
    "        \n",
    "        # Pad labels to max length in batch\n",
    "        labels = item['labels']\n",
    "        padded_labels = torch.full((max_label_length,), -100, dtype=torch.long)\n",
    "        padded_labels[:labels.size(0)] = labels\n",
    "        batched_data['labels'].append(padded_labels)\n",
    "\n",
    "    # Stack tensors\n",
    "    batched_data['pixel_values'] = torch.stack(batched_data['pixel_values'])\n",
    "    batched_data['labels'] = torch.stack(batched_data['labels'])\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageDataset(\"/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test\", x_dict, y_dict, processor)\n",
    "\n",
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
